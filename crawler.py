#-*- coding: utf-8 -*-
#
# Create by yunqian.
#
# Last updated: 2015-11-10
#
# google search results crawler

#字符  十进制  转义字符
#"  &#34;  &quot;
#&  &#38;  &amp;
#<  &#60;  &lt;
#>  &#62;  &gt;

import sys
reload(sys)
sys.setdefaultencoding('utf-8')

import urllib2, socket, time
import gzip, StringIO
import re, random, types
import re
import time

filenameinput='crawlvul.txt'
filenameall='crawlvulall.txt'
urlinput='https://www.exploit-db.com/webapps/'

def crawlwebapp(weburl,filename1,filename2):
    base_url='https://www.exploit-db.com/'
    url=weburl
#    url='https://www.exploit-db.com/webapps/'
    request = urllib2.Request(url)

    user_agent ='Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.2.32) Gecko/20120529 Firefox/3.6.32 (Palemoon/3.6.32)'
    request.add_header('User-agent', user_agent)
    request.add_header('connection','keep-alive')
    request.add_header('Accept-Encoding', 'gzip')
    request.add_header('referer', base_url)

    response = urllib2.urlopen(request)
    #print response.info()
    html = response.read()
    if(response.headers.get('content-encoding', None) == 'gzip'):
        html = gzip.GzipFile(fileobj=StringIO.StringIO(html)).read()
#print html
#1、提取网页中的相关数据
    myItems = re.findall('<tr>.*?<td.*?>(.*?)</td>.*?<td.*?<td.*?>(.*?)</td>.*?<td.*?title=(.*?)/>.*?<td.*?<a.*?>(.*?)</a>.*?<td.*?<a.*?>(.*?)</a>.*?<td.*?<a.*?>(.*?)</a>.*?</tr>',html,re.S)

    itemdate=[]
    itemdownload=[]
    itemverify=[]
    itemapp=[]
    itemtype=[]
    itemname=[]
    for item in myItems:
        #1、提取软件的发布日期,清除提取的日期里的空格符号
        temp1=item[0].strip()
        itemdate.append(temp1)
        #2、是否提供软件下载内容
        temp2=item[1].strip()
        if temp2!='-':
           temp2 = 'd'
        else:
            temp2 = '-'
        itemdownload.append(temp2)
        #3、该漏洞是否通过审核
        temp3=item[2].strip()
        temp3=temp3.strip('"')
        itemverify.append(temp3)
        #4、该漏洞的名称,清除转意符
        temp4=item[3].strip()
        temp4=temp4.replace('&lt;','<')
        temp4=temp4.replace('&amp;','&')
        temp4=temp4.replace('&gt;','>')
        temp4=temp4.replace('&quot;','"')
        itemapp.append(temp4)
        #5、该漏洞所在平台类型
        temp5=item[4].strip()
        itemtype.append(temp5)
        #6、发布该漏洞的作者
        temp6=item[5].strip()
        itemname.append(temp6)

        #7将所有的记录保存到文件中
        filevulall = open(filename2, 'a')
        filevulall.write(temp1 + ',' + temp2 + ',' + temp3 + ',' + temp4 + ',' + temp5 + ',' + temp6 + '\n')
        filevulall.close()

    print itemdate
    print itemdownload
    print itemverify
#print itemapp
    print itemtype
    print itemname
    print itemapp
#3写入文件
#    将漏洞名称写入到一个文件中file = open('crawl.txt', 'a')
    file = open(filename1, 'a')
    for item in itemapp:
        file.write(item + '\n')
    file.close()

    urlreturn=''
#print itemdownloadg
#2、获取下一个网页的链接
    try:
        urlitemx=re.findall('<a href=.*?</a>.*?<a class=.*?<a class="color" href="(.*?)">next</a>',html,re.S)
        urlreturn=urlitemx[0]
    except IndexError, e:
        urlreturn=''

    return urlreturn

#mian program ________  excute the program
while(urlinput!=''):
    oldurl=urlinput
    try:
        urltemp=crawlwebapp(urlinput,filenameinput,filenameall)
    except urllib2.URLError, e:
     #首先解释下URLError可能产生的原因：
     # #网络无连接，即本机无法上网
     # #连接不到特定的服务器
     # #服务器不存在
     #将下一个地址仍然保存为现有的地址，继续连接服务器，获取数据
        print e.reason
        urltemp=urlinput
     #记录出现问题的URL
        fileurlexcept = open('urlexception.txt', 'a')
        fileurlexcept.write(urlinput + '\n')
        fileurlexcept.close()

    #更新URL
    urlinput=urltemp
    print urlinput

    #如果正常连接到服务器，那么休眠0.5秒，否则休眠15秒
    if urlinput==oldurl:
        time.sleep(15) # 休眠15秒
    else:
        time.sleep(1) # 休眠1秒


